#!/usr/bin/env python3
"""
GPU ä¼˜åŒ–ç‰ˆæœ¬çš„ SimulStreaming æµ‹è¯•

åœ¨æ˜¾å­˜æœ‰é™çš„æƒ…å†µä¸‹ä½¿ç”¨æ›´å°çš„æ¨¡å‹å’Œä¼˜åŒ–é…ç½®
"""

import asyncio
import numpy as np
import time
import torch
from loguru import logger

# Setup logging
logger.add("simulstreaming_gpu_test.log", level="DEBUG")

from src.providers.base import AudioChunk
from src.providers.stt.simulstreaming_provider import SimulStreamingProvider


async def test_initialization():
    """Test provider initialization with GPU memory optimization"""
    logger.info("=" * 60)
    logger.info("Test: GPU Memory Optimized Initialization")
    logger.info("=" * 60)

    # æ£€æŸ¥ GPU å¯ç”¨æ˜¾å­˜
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_free = (torch.cuda.get_device_properties(0).total_memory -
                   torch.cuda.memory_allocated(0)) / 1024**3
        logger.info(f"GPU æ€»æ˜¾å­˜: {gpu_mem:.2f} GiB")
        logger.info(f"GPU å¯ç”¨æ˜¾å­˜: {gpu_free:.2f} GiB")

        # æ¸…ç†ç¼“å­˜
        torch.cuda.empty_cache()
        logger.info("å·²æ¸…ç† CUDA ç¼“å­˜")

    # æ˜¾å­˜ä¼˜åŒ–é…ç½®
    config = {
        # ä½¿ç”¨ small æ¨¡å‹ (~1GB vs large-v3 ~3GB)
        "model_size": "small",
        "model_path": "./small.pt",

        # GPU é…ç½®
        "device": "cuda",
        "compute_type": "int8",  # INT8 é‡åŒ–é™ä½ ~50% æ˜¾å­˜

        # AlignAtt é…ç½®
        "frame_threshold": 30,
        "beam_size": 1,  # Greedy è§£ç é™ä½æ˜¾å­˜

        # éŸ³é¢‘ç¼“å†²é…ç½®
        "audio_max_len": 20.0,  # é™ä½æœ€å¤§ç¼“å†² (30s â†’ 20s)
        "audio_min_len": 0.3,
        "min_chunk_size": 0.3,

        # ç¦ç”¨ VAD ç®€åŒ–æµ‹è¯•
        "vad_enabled": False,

        # è¯­è¨€
        "language": "zh",
    }

    provider = SimulStreamingProvider(config)

    try:
        logger.info("åˆå§‹åŒ– provider (medium æ¨¡å‹ + INT8 é‡åŒ–)...")
        start = time.time()
        await provider.initialize()
        elapsed = time.time() - start

        logger.info(f"âœ… åˆå§‹åŒ–æˆåŠŸï¼Œè€—æ—¶ {elapsed:.2f}s")
        logger.info(f"Provider: {provider}")

        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            logger.info(f"GPU å·²åˆ†é…æ˜¾å­˜: {allocated:.2f} GiB")
            logger.info(f"GPU å·²ä¿ç•™æ˜¾å­˜: {reserved:.2f} GiB")

        # Health check
        is_healthy = await provider.health_check()
        logger.info(f"å¥åº·æ£€æŸ¥: {'âœ… Healthy' if is_healthy else 'âŒ Unhealthy'}")

        return provider

    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


async def test_quick_transcription(provider: SimulStreamingProvider):
    """å¿«é€Ÿè½¬å½•æµ‹è¯•"""
    logger.info("\n" + "=" * 60)
    logger.info("Test: Quick Transcription")
    logger.info("=" * 60)

    # ç”Ÿæˆ 1 ç§’é™éŸ³æµ‹è¯•
    sample_rate = 16000
    duration_s = 1.0
    audio_data = np.zeros(int(sample_rate * duration_s), dtype=np.float32)
    audio_int16 = (audio_data * 32768).astype(np.int16)

    chunk = AudioChunk(
        data=audio_int16.tobytes(),
        sample_rate=sample_rate,
        timestamp=time.time(),
        channels=1,
        format="int16",
    )

    try:
        start = time.time()
        result = await provider.transcribe(chunk, language="zh")
        elapsed = time.time() - start

        logger.info(f"â±ï¸ è½¬å½•è€—æ—¶: {elapsed:.3f}s")
        logger.info(f"ğŸ“ ç»“æœ: {result}")
        logger.info(f"âœ… è½¬å½•æˆåŠŸ")

    except Exception as e:
        logger.error(f"âŒ è½¬å½•å¤±è´¥: {e}")
        raise


async def test_cleanup(provider: SimulStreamingProvider):
    """æ¸…ç†æµ‹è¯•"""
    logger.info("\n" + "=" * 60)
    logger.info("Test: Cleanup")
    logger.info("=" * 60)

    try:
        await provider.cleanup()

        # æ¸…ç† GPU ç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info("âœ… æ¸…ç†æˆåŠŸ")

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        raise


async def main():
    """è¿è¡Œä¼˜åŒ–æµ‹è¯•"""
    logger.info("ğŸš€ SimulStreaming GPU ä¼˜åŒ–æµ‹è¯•")
    logger.info("=" * 60)

    try:
        # åˆå§‹åŒ–
        provider = await test_initialization()

        # å¿«é€Ÿè½¬å½•
        await test_quick_transcription(provider)

        # æ¸…ç†
        await test_cleanup(provider)

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        logger.info("=" * 60)

        logger.info("\nğŸ’¡ æç¤º:")
        logger.info("- å¦‚æœä»ç„¶æ˜¾å­˜ä¸è¶³ï¼Œè¯·å…³é—­å…¶ä»– GPU è¿›ç¨‹")
        logger.info("- æˆ–è€…ä½¿ç”¨ CPU æ¨¡å¼: device='cpu'")
        logger.info("- medium æ¨¡å‹æ˜¾å­˜éœ€æ±‚: ~1.5-2GB")
        logger.info("- large-v3 æ¨¡å‹æ˜¾å­˜éœ€æ±‚: ~3-4GB")

    except Exception as e:
        logger.error(f"\n{'=' * 60}")
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"{'=' * 60}")

        logger.error("\nğŸ”§ æ•…éšœæ’é™¤:")
        logger.error("1. å…³é—­å…¶ä»–å ç”¨ GPU çš„è¿›ç¨‹")
        logger.error("2. ä½¿ç”¨ device='cpu' åˆ‡æ¢åˆ° CPU æ¨¡å¼")
        logger.error("3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹: 'small' æˆ– 'base'")
        logger.error("4. è®¾ç½®ç¯å¢ƒå˜é‡: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")

        raise


if __name__ == "__main__":
    asyncio.run(main())
